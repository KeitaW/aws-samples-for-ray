{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example has been adapted to use the [Women's Clothing Review Dataset](https://github.com/AFAgarap/ecommerce-reviews-analysis/blob/7062594/Womens%20Clothing%20E-Commerce%20Reviews.csv) from https://github.com/ray-project/ray/blob/b9a4f64/python/ray/train/examples/transformers/transformers_example.py (which was adapted to use Ray Train from https://github.com/huggingface/transformers/blob/75259b4/examples/pytorch/text-classification/run_glue_no_trainer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install torch transformers pandas datasets accelerate scikit-learn mlflow tensorboard ray[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel to pick up the pip installs above\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "import ray\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, load_metric\n",
    "from ray.train import Trainer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"roberta-base\"\n",
    "use_slow_tokenizer = False\n",
    "per_device_train_batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.0\n",
    "num_train_epochs = 3\n",
    "max_train_steps = 100\n",
    "gradient_accumulation_steps = 1\n",
    "lr_scheduler_type = \"linear\"\n",
    "num_warmup_steps = 0\n",
    "output_dir = None\n",
    "seed = None\n",
    "num_workers = 1\n",
    "use_gpu = False\n",
    "\n",
    "data_files = {}\n",
    "data_files[\"train\"] = \"Womens Clothing E-Commerce Reviews.csv\"\n",
    "extension = \"csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func():\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    print(accelerator.state)\n",
    "\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "\n",
    "    label_list = raw_datasets[\"train\"].unique(\"sentiment\")\n",
    "    label_list.sort()  # Sort for determinism\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path, num_labels=num_labels, \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path, use_fast=not use_slow_tokenizer\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    sentence1_key, sentence2_key = \"review_body\", None\n",
    "\n",
    "    label_to_id = None\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "    if label_to_id is not None:\n",
    "        model.config.label2id = label_to_id\n",
    "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "    padding = \"max_length\" if pad_to_max_length else False\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        texts = (\n",
    "            (examples[sentence1_key],)\n",
    "            if sentence2_key is None\n",
    "            else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(\n",
    "            *texts, padding=padding, max_length=max_length, truncation=True\n",
    "        )\n",
    "\n",
    "        if \"sentiment\" in examples:\n",
    "            if label_to_id is not None:\n",
    "                result[\"labels\"] = [\n",
    "                    label_to_id[l] for l in examples[\"sentiment\"]\n",
    "                ]\n",
    "            else:\n",
    "                result[\"labels\"] = examples[\"sentiment\"]\n",
    "\n",
    "        return result\n",
    "\n",
    "    processed_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        print(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    if pad_to_max_length:\n",
    "        data_collator = default_data_collator\n",
    "    else:\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "        )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "    )\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "    model, optimizer, train_dataloader = accelerator.prepare(\n",
    "       model, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / gradient_accumulation_steps\n",
    "    )\n",
    "    if max_train_steps is None:\n",
    "        max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    else:\n",
    "        num_train_epochs = math.ceil(\n",
    "            max_train_steps / num_update_steps_per_epoch\n",
    "        )\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "    metric = load_metric(\"accuracy\")\n",
    "\n",
    "    total_batch_size = (\n",
    "        per_device_train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    print(\"***** Training *****\")\n",
    "    print(f\"  Num examples = {len(train_dataset)}\")\n",
    "    print(f\"  Num epochs = {num_train_epochs}\")\n",
    "    print(\n",
    "        f\"  Instantaneous batch size per device =\"\n",
    "        f\" {per_device_train_batch_size}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Total train batch size (w. parallel, distributed & accumulation) \"\n",
    "        f\"= {total_batch_size}\"\n",
    "    )\n",
    "    print(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
    "    print(f\"  Total optimization steps = {max_train_steps}\")\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        range(max_train_steps), disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "    completed_steps = 0\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if (\n",
    "                step % gradient_accumulation_steps == 0\n",
    "                or step == len(train_dataloader) - 1\n",
    "            ):\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= max_train_steps:\n",
    "                break\n",
    "        \n",
    "    if output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to a Ray cluster for distributed training.\n",
    "ray.shutdown()\n",
    "ray.init(address=\"ray://localhost:10001\",\n",
    "         runtime_env={\"pip\": [\n",
    "                                \"torch\", \n",
    "                                \"scikit-learn\",\n",
    "                                \"transformers\",\n",
    "                                \"pandas\",\n",
    "                                \"datasets\",\n",
    "                                \"accelerate\",\n",
    "                                \"scikit-learn\",\n",
    "                                \"mlflow\", \n",
    "                                \"tensorboard\"                         \n",
    "                             ]\n",
    "                     }\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\"torch\", num_workers=num_workers, use_gpu=use_gpu,\n",
    "                  resources_per_worker={'CPU': 4, 'GPU': 0})\n",
    "trainer.start()\n",
    "trainer.run(train_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
