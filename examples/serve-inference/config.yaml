# This file was generated using the `serve build` command on Ray v2.7.1.

proxy_location: EveryNode

http_options:

  host: 0.0.0.0

  port: 8000

grpc_options:

  port: 9000

  grpc_servicer_functions: []

applications:

- name: app1

  route_prefix: /

  import_path: aws_neuron_core_inference_serve:app

  runtime_env: {}

  deployments:

  - name: LlamaModel
    autoscaling_config:
      min_replicas: 1
      initial_replicas: null
      max_replicas: 4
      target_num_ongoing_requests_per_replica: 1.0
      metrics_interval_s: 10.0
      look_back_period_s: 30.0
      smoothing_factor: 1.0
      upscale_smoothing_factor: null
      downscale_smoothing_factor: null
      downscale_delay_s: 600.0
      upscale_delay_s: 30.0
    ray_actor_options:
      runtime_env:
        env_vars:
          NEURON_CC_FLAGS: -O1
          NEURON_COMPILE_CACHE_URL: /home/ubuntu/neuron_demo/neuron-compile-cache
      num_cpus: 1.0
      resources:
        neuron_cores: 12

  - name: APIIngress
    num_replicas: 2
